# W4

## Bia-Variance Trade-off

![截屏2021-04-28 09.45.31](/Users/chixinning/Library/Application Support/typora-user-images/截屏2021-04-28 09.45.31.png)

![截屏2021-04-28 09.46.54](/Users/chixinning/Library/Application Support/typora-user-images/截屏2021-04-28 09.46.54.png)

我们想让模型既准又不会换一部数据废掉。

## Feature Selection

- Predictive Accuracy
- Interpretability，在金融领域很重要，而对于一些Technology Company在意的程度会少一点。

### 如何make sense的进行feature selection？

1. T-test：

   1. Select n features with the best t stats
   2. Select all features with significant p-value

   问题：对于t-test本身质疑，不同variable之间t-test不能被比较(保留)

2. R-square：

   1. Forward Stepwise

      从前往后加，1个feature,2个

      3个features:第一个进去，第一个是最好的，第二个进去，也是好的，但我们忽略了2和3features的combination是好的的情况。

   2. Backward Stepwise

      从100个features一个个删feature

   问题：这个方法存在assumption：我们看到的数据在sample里的R-square是最好的,可能换一批数据他的combination更好是不知道的？![截屏2021-04-28 10.11.53](/Users/chixinning/Library/Application Support/typora-user-images/截屏2021-04-28 10.11.53.png)

   ![截屏2021-04-28 10.12.09](/Users/chixinning/Library/Application Support/typora-user-images/截屏2021-04-28 10.12.09.png)

3. Regularization：通过惩罚少选features

## Regularization

$R-square$与$Adjusted-R-square$不是一个特别好的惩罚，但其中的惩罚思想比较好。

惩罚：只要我们加新的feature(x/$\beta$)进去，我们模型就加惩罚。

![截屏2021-04-28 10.16.37](/Users/chixinning/Library/Application Support/typora-user-images/截屏2021-04-28 10.16.37.png)

这里penalty项前的$\lambda$可以理解成惩罚的力度。

如果$\lambda$为0，这个惩罚是大还是小？当$\lambda$越来越大的时候，$\hat{\beta}$会变成什么样子才能使目标函数越来越趋近于最小化。如果penalty越来越大，beta会变成什么样子？

极端情况：

1. $\lambda=0$，没有惩罚
2. $\lambda->+\infin$,Object Function肯定会变大，这个时候为了让它打到最小化，所以惩罚大的意思理解是为了让Object Function达到最小化，会有越来越多的$\beta$为0.即我们越来越不容易让变量加进去。

> $\lambda$越小，variable的数量越少。

加penalty之前Object Function是可以变小的，这里加了penalty让它变大了，这里只是balance一下整体的New Object Function以达到最小值。

## Lasso and Ridge Model

### Ridge Model:L2-penalty

NiLv老师上课:

定义称:$\hat{\beta(k)}=(X^TX+kI)^{-1}X^Ty$为回归系数$\beta$的Ridge Estimator

从另一个角度看岭回归估计：由于$\hat{\beta_{OLS}}$是最小化离差平方和，所以可以证明$\hat{\beta(k)}$是最小化带有$L_2$正则项的离差平方和的解。

$\hat{\beta_{OLS}}=arg min(y-X\beta)^T(y-X\beta)+\lambda\beta^T\beta$

### Lasso Regression:L1-penalty

![截屏2021-04-28 10.33.36](/Users/chixinning/Library/Application Support/typora-user-images/截屏2021-04-28 10.33.36.png)

上下两个Optimization Function是等价的。

### Ridge v.s. Lasso

![截屏2021-04-28 10.36.40](/Users/chixinning/Library/Application Support/typora-user-images/截屏2021-04-28 10.36.40.png)

这个subject to这个蓝绿色的正圆型，交叉点就是解。

可以看出，Ridge的交点里,$\beta_1$非常的靠近于0，即非常小，但$\beta_1$是不等于0的。

![截屏2021-04-28 10.38.14](/Users/chixinning/Library/Application Support/typora-user-images/截屏2021-04-28 10.38.14.png)

$\beta_1=0$,即从二维的角度看Lasso会让一些$\beta$直接等于0的。

$Lasso$直接帮我将一些作废了。

| 区别  | Lasso                                       | Ridge                                                        |
| ----- | ------------------------------------------- | ------------------------------------------------------------ |
| 区别1 | 会让一些$\beta$直接等于0的，模型可interpret | Ridge的交点里,$\beta_1$非常的靠近于0，即非常小，但$\beta_1$是不等于0的。 |
| 区别2 | 带绝对值的一般没有显式解【数学没学好:D      | 有显示解。                                                   |
| 区别3 | Not Robust                                  | Robust                                                       |

