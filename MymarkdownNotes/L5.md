# L5

- Learning Curve:sklearn alpha即是lambda

  X轴是参数，y轴是最终要变小的RMSE

- 这个图对吗？

![截屏2021-05-01 09.17.28](https://tva1.sinaimg.cn/large/008i3skNgy1gq2nunrxx3j30qi0i2gny.jpg)

【In Sample Overfitting】

这个是$traingSet$的RMSE，每一步不断地问自己为什么是这样的？self-checking matters in work.

所以这个图

> $traingSet$不要惩罚，整个模型Overfitting是最好的。
>
> 所以$traingSet$这个图非常的符合我的expectation！

![截屏2021-05-01 09.21.11](https://tva1.sinaimg.cn/large/008i3skNgy1gq2nyg80zsj30qi0i2mzv.jpg)

这个testSet才是我们所说的长得是U型的。

![截屏2021-05-01 09.23.20](https://tva1.sinaimg.cn/large/008i3skNgy1gq2o0okieuj30qi030760.jpg)

这里还有一个新问题，这里是随机的。

仅根据这30%的数据科学吗？

> Estimation Heavily depends on test-set

## Cross Validation

### K-Fold Cross-Validation

平均分成k份一样的sets，

第k个做trainingSet,剩下的做testSet

找到平均之下哪一个是最优的，避免我们依赖某一个testSet的分法。

```python
ridgeModel = RidgeCV(alphas = [alphaBest*i for i in temp], cv = 5)
```

这里alphaLIst可以先宽泛再逐步细化，比如先选出10，再在10附近选出12这样。


```python
alphaList = [0.01, 0.03, 0.06, 0.1, 0.3, 0.6, 1, 3, 6, 10, 20, 30, 40, 50, 60]
```

### make Sense吗？

![截屏2021-05-01 09.33.49](https://tva1.sinaimg.cn/large/008i3skNgy1gq2obmhczsj30di016q3h.jpg)

因为Ridge是圆，是不消灭的，只是把一些beta变小，而不是变成0.所以这里也是一处self-checking

### Lasso Model的运行结果

在这里Ridge比Lasso效果好，用Ridge做第一次尝试提交结果，这个结果基本是ok的。

## PCA

PCA做的东西与Ridge模型殊途同归，但它从不同的思考方式进行dimension reduction。但是PCA在日常有他不好用的地方，即它难以解释，难以解释PC1是什么，只能用PC1和其中一个feature进行corr，但它其实不代表什么，他就是PC1.

![截屏2021-05-01 09.45.06](https://tva1.sinaimg.cn/large/008i3skNgy1gq2onc7886j30o807aab9.jpg)

用一维坐标代表二维的点？

在v2这个新的坐标之下没有这么分散。此时不如把v2全部投影到v1上，此时我们就只用了一个dimension。![截屏2021-05-01 09.46.59](https://tva1.sinaimg.cn/large/008i3skNgy1gq2op9xjxvj30cw0d474v.jpg)

$v_1$和$v_2$所学到的。

python中的PCA不标准化，但是标准的PCA是有进行标准化这一步的。

用PCA后的87个feature因为已经不存在多重共线性了，所以不需要ridge了，而是直接线性回归即可。

PCA第一步就已经是点的投射，此时就没有实际意义了。