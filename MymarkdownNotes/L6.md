# L6

## 整合模型--Ensemble Method

为什么整合模型更好？

普通模型有一些假设

不要把鸡蛋放在同一个篮子里面。

## Bagging

对树模型->随机森林？(Boostraping Idea)

我们的数据量总是有限的。我们使用放回抽样抽100次，重新抽出来成为一个新的Histogram。

所以Boostrap主要解决样本数量不足的情况。

Bagging idea:

to get a set of trees, randomize the data
use bootstrapping to get new datasets
in bootstrapping, we create a new dataset by sampling with replacement from our current dataset
do this K times
Fit a tree to every new dataset
new estimator is average of bootstrap estimators

## Boosting->XGBoost

- Strong learner v.s. weak learner
- A strong learner is a method that can learn a decision rule arbitrarily well.
  A weak learner is a simple method that does better than guessing, but cannot learn a decision rule arbitrarily well.
- 把weak learner combine起来变成strong learner
- ![Screen Shot 2021-05-22 at 09.20.27](https://tva1.sinaimg.cn/large/008i3skNly1gqqxy8hkyfj30zu09m0xf.jpg)

![Screen Shot 2021-05-22 at 09.21.38](https://tva1.sinaimg.cn/large/008i3skNly1gqqxzbwoisj30zu0900wy.jpg)

第一次树模型有三个error,第二次输模型我们只把预测不对的挑出来。

![Screen Shot 2021-05-22 at 09.23.53](https://tva1.sinaimg.cn/large/008i3skNly1gqqy1pfisgj30zu08u43h.jpg)

不断调整预测不对的数据(weighted average)

Boosting 之间的区别：猜对的和错的weight不一样，树之间的weight不一样】

Q：特征工程//超参数？

boosting和bagging区别，boosting 更针对我之前预测不准的地方

## Adaboost Adaptive Boosting