{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python386jvsc74a57bd00adcc2737ebf6a4a119f135174df96668767fca1ef1112612db5ecadf2b6d608",
   "display_name": "Python 3.8.6 64-bit"
  },
  "metadata": {
   "interpreter": {
    "hash": "0adcc2737ebf6a4a119f135174df96668767fca1ef1112612db5ecadf2b6d608"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##COPYRIGHT from 仲益教育\n",
    "import sys\n",
    "# sys.path.append(\"C://Users//Haipxiang He//desktop//Kaggle//Lecture 2\")\n",
    "#conda install mxltend --channel conda-forge\n",
    "#pip install package name\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import f_regression, SelectKBest\n",
    "import numpy as np\n",
    "from Help_functions_v2 import sklearn_Pvalue, sklearn_adjR2, RMSE\n",
    "\n",
    "#1: import data\n",
    "train_data=pd.read_csv(\"/Users/chixinning/Desktop/kaggle/KaggleMyOwn/Mycode/raw_data/train_clean_teacher_2.csv\")\n",
    "\n",
    "#2.1: feature selection with p_value\n",
    "train_data.index=train_data[\"Id\"]\n",
    "train_data.drop(\"Id\", axis=1, inplace=True)\n",
    "dependentV=train_data[\"SalePrice\"]\n",
    "train_data.drop(\"SalePrice\", axis=1, inplace=True)\n",
    "#Partition the dataset in train + validation sets\n",
    "#usually linear regression needs at least 30 observations\n",
    "#split of train and validation can be 70:30, or 60:40\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_data, dependentV, test_size = 0.3, random_state = 0)\n",
    "print(\"X_train : \" + str(X_train.shape))\n",
    "print(\"X_test : \" + str(X_test.shape))\n",
    "print(\"y_train : \" + str(y_train.shape))\n",
    "print(\"y_test : \" + str(y_test.shape))\n",
    "\n",
    "#2.1.1 select top 20 features with the best F-stats\n",
    "X_scored = SelectKBest(score_func=f_regression, k=20)\n",
    "X_scored.fit(X_train, y_train)\n",
    "feat_list=X_scored.get_support()\n",
    "\n",
    "feature_scoring = pd.DataFrame({\n",
    "        'feature': X_train.columns[feat_list],\n",
    "        'pvalue': X_scored.pvalues_[feat_list]\n",
    "    })\n",
    "\n",
    "print(feature_scoring)\n",
    "#homework, find out the RMSE of in sample and out of sample regression of the top 20 selected features\n",
    "\n",
    "#2.1.2 select all features with individual p_value <=0.05\n",
    "X_scored2 = SelectKBest(score_func=f_regression, k='all').fit(X_train, y_train)\n",
    "\n",
    "feature_scoring2 = pd.DataFrame({\n",
    "        'feature': X_train.columns,\n",
    "        'pvalue': X_scored2.pvalues_\n",
    "    })\n",
    "feat_pvalue_significant=feature_scoring[feature_scoring2.pvalue<=0.05]\n",
    "feat_pvalue_significant['feature'].values\n",
    "\n",
    "#2.1.3 select N features based on the \n",
    "#2.2 feature selection based on forward/backward elimination based on R Square\n",
    "from mlxtend.feature_selection import SequentialFeatureSelector as SFS\n",
    "#2.2.1 find the best k features using stepforward method\n",
    "stepforward = SFS(LinearRegression(), \n",
    "           k_features=10, \n",
    "           forward=True, \n",
    "           floating=False, \n",
    "           verbose=2,\n",
    "           scoring='r2',\n",
    "           cv=0)\n",
    "\n",
    "stepforward = stepforward.fit(np.array(X_train), y_train)\n",
    "print(X_train.columns[list(stepforward.k_feature_idx_)])\n",
    "#homework, find out the RMSE of in sample and out of sample regression of selected features \n",
    "#using forward elimination\n",
    "\n",
    "#2.2.2 find the best k features using stepbackward method\n",
    "backward = SFS(LinearRegression(), \n",
    "           k_features=10, \n",
    "           forward=False, \n",
    "           floating=False, \n",
    "           verbose=2,\n",
    "           scoring='r2',\n",
    "           cv=0)\n",
    "\n",
    "backward = stepforward.fit(np.array(X_train), y_train)\n",
    "print(X_train.columns[list(backward.k_feature_idx_)])\n",
    "\n",
    "\n",
    "#3 feature selection with regularization\n",
    "#3.1 Ridge Regularization\n",
    "ridge_model=Ridge(alpha=1)\n",
    "ridge_model.fit(X_train, y_train)\n",
    "y_trainPred=ridge_model.predict(X_train)\n",
    "#in sample\n",
    "print(\"in-sample r-squared is\")\n",
    "print(r2_score(y_train, y_trainPred))\n",
    "print (\"RMSE is of in-sample\")\n",
    "print(RMSE(y_trainPred,y_train))\n",
    "#out of sample\n",
    "y_testPred=ridge_model.predict(X_test)\n",
    "print(\"out-of-sample r-squared is\")\n",
    "print(r2_score(y_test, y_testPred))\n",
    "print (\"RMSE is out-of-sample\")\n",
    "print(RMSE(y_testPred,y_test))\n",
    "\n",
    "#3.2 Lasso Regularization\n",
    "lasso_model=Lasso(alpha=1)\n",
    "lasso_model.fit(X_train, y_train)\n",
    "y_trainPred=lasso_model.predict(X_train)\n",
    "#in sample\n",
    "print(\"lasso in-sample r-squared is\")\n",
    "print(r2_score(y_train, y_trainPred))\n",
    "print (\"Lasso RMSE of in-sample is \")\n",
    "print(RMSE(y_trainPred,y_train))\n",
    "#out of sample\n",
    "y_testPred=lasso_model.predict(X_test)\n",
    "print(\"Lasso out-of-sample r-squared is\")\n",
    "print(r2_score(y_test, y_testPred))\n",
    "print (\"Lasso RMSE out-of-sample is\")\n",
    "print(RMSE(y_testPred,y_test))\n"
   ]
  }
 ]
}